# CART (Decision Tree)
`r Sys.Date()` updated  
Ref: [here](https://www.r-bloggers.com/2021/04/decision-trees-in-r/)  
Ref: [random forest](https://lgatto.github.io/IntroMachineLearningWithR/supervised-learning.html#scaling-and-centering)  
Decision Tree can handle classification and regression problem. It is also called CART (Classification and Regression Tree). In the following, we will use spam data and Boston house pricing data to demonstrate.  

## Classification Tree
```{r}
library(DAAG) # data sets used in examples and exercise
library(party) # for recursive partitioning
library(rpart) # recursive partitioning and regression trees
library(rpart.plot) # plot rpart models
library(mlbench) # collection of ML benchmark problems
library(caret) # Misc functions for training/plotting models
library(pROC)
library(tree) # classification and regression tree
```
```{r}
# Getting Email spam data
str(spam7)
```
```{r}
# Data partition
set.seed(1234)
mydata <- spam7
ind <- sample(2, nrow(mydata), replace=TRUE, prob=c(0.5,0.5)) # assign index
train <- mydata[ind == 1,]
test <- mydata[ind == 2,]
tree <- rpart(yesno ~ ., data=train)
rpart.plot(tree)
printcp(tree)
plotcp(tree)
```
```{r}
# print confusion matrix of your model on training dataset
p <- predict(tree, train, type = "class")
confusionMatrix(p, train$yesno, positive="y")
```
```{r message=FALSE, warning=FALSE}
# ROC curve
p1 <- predict(tree, test, type="prob")
p1 <- p1[,2] # extract y information
r <- multiclass.roc(test$yesno, p1, percent = TRUE)
roc <- r[["rocs"]]
r1 <- roc[[1]]
plot.roc(
    r1,
    print.auc = TRUE,
    auc.polygon = TRUE,
    grid = c(0.1, 0.2),
    grid.col = c("green", "red"),
    max.auc.polygon = TRUE,
    auc.polygon.col = "lightblue",
    print.thres = TRUE,
    main = "ROC Curve"
)
```
## Regression Tree
```{r}
data("BostonHousing")
mydata <- BostonHousing
str(mydata)
```
```{r}
# Data partition
set.seed(1234)
ind <- sample(2, nrow(mydata), replace = T, prob = c(0.5, 0.5))
train <- mydata[ind == 1,]
test <- mydata[ind == 2,]

tree <- rpart(medv ~., data = train)
rpart.plot(tree)
printcp(tree)
plotcp(tree)
```
```{r}
# predict
p <- predict(tree, train)

# RMSE
(sqrt(mean((train$medv - p)^2)))

# R squared
(cor(train$medv, p))^2
```
In the regression model, the r square value is 80% and RMSE is 4.13, not bad at all.. In this way, you can make use of Decision classification regression tree models.  

## Random Forest  
```{r}
library(mlbench)
data(Sonar)
(head(Sonar))

set.seed(12)
model <- train(
    Class ~.,
    data = Sonar,
    method = "ranger"
)
```
```{r}
print(model)
```
```{r}
plot(model)
```
Tune hyperparameters:
```{r}
set.seed(42)
myGrid <- expand.grid(
    mtry = c(5, 10, 20, 40, 60),
    splitrule=c("gini", "extratrees"),
    min.node.size = 1
)

model <- train(
    Class ~ .,
    data = Sonar,
    method = "ranger",
    tuneGrid = myGrid,
    trControl = trainControl(
        method = "cv",
        number = 5,
        verboseIter = FALSE
    )
)

print(model)
```
```{r}
plot(model)
```

