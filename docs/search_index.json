[["index.html", "R algorithm notes Chapter 1 About", " R algorithm notes Yao-Chung 2021-12-25 Chapter 1 About This notebook is created for recording algorithms written in R, as well as its application timing and examples. "],["ab-test.html", "Chapter 2 A/B test 2.1 Scenario 2.2 Case study", " Chapter 2 A/B test 2021-12-25 updated A/B test is used for testing which version of product can give you a preferable outcome by setting two versions of a product/test with only one variable is different (which is version A and version B). A is usually the “current version” and B is often the “improved version”. 2.1 Scenario For someone who wants to test new feature on product. A: Offer ends this Saturday! Please use promotion code A B: The offer is about to end, please use promotion code B Send these message to a population and see which one get more clicks or sell more products. For a more persuasive result, a statistical test should be used to test if it is a significant result. 2.2 Case study ref In short, we want to test two variant design on a hotel booking website. Based on result, make recommendation to your stakeholders. library(tidyverse) urlfile &lt;- &quot;https://raw.githubusercontent.com/etomaa/A-B-Testing/master/data/Website%20Results.csv&quot; df &lt;- read_csv(url(urlfile), show_col_types = FALSE) glimpse(df) ## Rows: 1,451 ## Columns: 4 ## $ variant &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;… ## $ converted &lt;lgl&gt; FALSE, FALSE, FALS… ## $ length_of_stay &lt;dbl&gt; 0, 0, 0, 0, 0, 0, … ## $ revenue &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0… variant A: current website (control group) variant B: experimental group (to see if new feature increase the conversion) converted: if TRUE, it means that the customer completes bookings and it’s going to show FALSE when the customer visits the sites but not makes a booking. Test Hypothesis: - Null hypothesis: Pcont_A = Pcont_B - Alternative hypothesis: Pcont_A != Pcont_B # conversion rate A conversion_subset_A &lt;- df %&gt;% filter(variant == &quot;A&quot; &amp; converted == &quot;TRUE&quot;) conversions_A &lt;- nrow(conversion_subset_A) visitors_A &lt;- nrow(df %&gt;% filter(variant == &quot;A&quot;)) conv_rate_A &lt;- (conversions_A/visitors_A) # conversion rate B conversion_subset_B &lt;- df %&gt;% filter(variant == &quot;B&quot; &amp; converted == &quot;TRUE&quot;) conversions_B &lt;- nrow(conversion_subset_B) visitors_B &lt;- nrow(df %&gt;% filter(variant == &quot;B&quot;)) conv_rate_B &lt;- (conversions_B/visitors_B) print(paste0(&quot;conversion rates of A and B are &quot;, round(conv_rate_A, 4), &quot; and &quot;, round(conv_rate_B, 4))) ## [1] &quot;conversion rates of A and B are 0.0277 and 0.0507&quot; Then we compute the relative uplift using the conversion rates A &amp; B. uplift &lt;- (conv_rate_B - conv_rate_A) / conv_rate_A * 100 uplift ## [1] 82.71918 B is better 83% than A. This is high enough to decide a winner. However, we need to use statistical methods to give a more precise result. # Pooled sample proportion for variants A &amp; B p_pool &lt;- (conversions_A + conversions_B) / (visitors_A + visitors_B) # Let&#39;s compute Standard error for variants A &amp; B (SE_pool) SE_pool &lt;- sqrt(p_pool * (1 - p_pool) * ((1 / visitors_A) + (1 / visitors_B))) # Let&#39;s compute the margin of error for the pool MOE &lt;- SE_pool * qnorm(0.975) # Point Estimate or Difference in proportion d_hat &lt;- conv_rate_B - conv_rate_A # compute the Z-score so we can determine the p-value z_score &lt;- d_hat / SE_pool cat(paste0(&quot;The pooled probabilty is &quot;, round(p_pool, 4), &quot;.\\nStandard error is &quot;, round(SE_pool, 4), &quot;.\\nMargin of error is &quot;, round(MOE, 4), &quot;.\\nDifference in proportion of variants A &amp; B is &quot;, round(d_hat,4), &quot;.\\nZ score is &quot;, round(z_score, 4))) ## The pooled probabilty is 0.0393. ## Standard error is 0.0102. ## Margin of error is 0.02. ## Difference in proportion of variants A &amp; B is 0.0229. ## Z score is 2.2495 Using Z-score, we can quickly determine the p-value via a look-up table, or using code below. Also conpute the confidence interval for the pool p_value &lt;- pnorm(q = -z_score, mean = 0, sd = 1) * 2 ci &lt;- c(d_hat - MOE, d_hat + MOE) # confidence interval of variant A X_hat_A &lt;- conversions_A / visitors_A se_hat_A &lt;- sqrt(X_hat_A * (1 - X_hat_A) / visitors_A) ci_A &lt;- c(X_hat_A - qnorm(0.975) * se_hat_A, X_hat_A + qnorm(0.975) * se_hat_A) # confidence interval of variant B X_hat_B &lt;- conversions_B / visitors_B se_hat_B &lt;- sqrt(X_hat_B * (1 - X_hat_B) / visitors_B) ci_B &lt;- c(X_hat_B - qnorm(0.975) * se_hat_B, X_hat_B + qnorm(0.975) * se_hat_B) cat(paste0(&quot;p value is &quot;, round(p_value, 4), &quot;\\nConfidence interval is &quot;, list(round(ci, 4)), &quot;\\nConfidence interval of A is &quot;, list(round(ci_A, 4)),&quot;\\nConfidence interval of B is &quot;, list(round(ci_B, 4)))) ## p value is 0.0245 ## Confidence interval is c(0.003, 0.0429) ## Confidence interval of A is c(0.0158, 0.0397) ## Confidence interval of B is c(0.0348, 0.0666) Show the computation result in table: vis_result_pool &lt;- data.frame( metric = c( &#39;Estimated Difference&#39;, &#39;Relative Uplift(%)&#39;, &#39;pooled sample proportion&#39;, &#39;Standard Error of Difference&#39;, &#39;z_score&#39;, &#39;p-value&#39;, &#39;Margin of Error&#39;, &#39;CI-lower&#39;, &#39;CI-upper&#39;), value = c( conv_rate_B - conv_rate_A, uplift, p_pool, SE_pool, z_score, p_value, MOE, ci[1], ci[2] )) vis_result_pool ## metric ## 1 Estimated Difference ## 2 Relative Uplift(%) ## 3 pooled sample proportion ## 4 Standard Error of Difference ## 5 z_score ## 6 p-value ## 7 Margin of Error ## 8 CI-lower ## 9 CI-upper ## value ## 1 0.022945680 ## 2 82.719178082 ## 3 0.039283253 ## 4 0.010200138 ## 5 2.249546089 ## 6 0.024477774 ## 7 0.019991903 ## 8 0.002953777 ## 9 0.042937584 Recommendation and Conclusion * Variant A has 20 conversions and 721 hits whereas Variant B has 37 conversions and 730 hits. * Relative uplift of 82.72% based on a variant A conversion rate is 2.77% and for B is 5.07%. Hence, variant B is better than A by 82.72%. * For this analysis P-value computed was 0.02448. Hence, there is strong statistical significance in test results. * From the above results that depict strong statistical significance. You should reject the null hypothesis and proceed with the launch. * Therefore, Accept Variant B and you can roll it to the users for 100%. Limitations It is one of the tools for conversion optimization and it’s not an independent solution and it’s not going to fix all the conversion issues of ours and it can’t fix the issues as you get with messy data and you need to perform more than just an A/B test to improve on conversions. "],["k-means.html", "Chapter 3 K-means 3.1 Case 1 3.2 Case 2 3.3 Hierarchical clustering", " Chapter 3 K-means 2021-12-25 updated 3.1 Case 1 K-means clustering: It is a method to cluster n points into k clusters based on the means of shortest distance. (cautsion: do not confused it with K-nearest clustering). We use the demo data sets “USArrests”. data(&quot;USArrests&quot;) df &lt;- USArrests head(df) ## Murder Assault UrbanPop Rape ## Alabama 13.2 236 58 21.2 ## Alaska 10.0 263 48 44.5 ## Arizona 8.1 294 80 31.0 ## Arkansas 8.8 190 50 19.5 ## California 9.0 276 91 40.6 ## Colorado 7.9 204 78 38.7 # scaling the data df &lt;- scale(df) head(df) ## Murder Assault UrbanPop ## Alabama 1.24256408 0.7828393 -0.5209066 ## Alaska 0.50786248 1.1068225 -1.2117642 ## Arizona 0.07163341 1.4788032 0.9989801 ## Arkansas 0.23234938 0.2308680 -1.0735927 ## California 0.27826823 1.2628144 1.7589234 ## Colorado 0.02571456 0.3988593 0.8608085 ## Rape ## Alabama -0.003416473 ## Alaska 2.484202941 ## Arizona 1.042878388 ## Arkansas -0.184916602 ## California 2.067820292 ## Colorado 1.864967207 We use factoextra package to create beautiful clusters visualization We also use fviz_nbclust to determine the optimal number of clusters set.seed(123) fviz_nbclust(df, kmeans, method=&quot;wss&quot;) + geom_vline(xintercept = 4, linetype = 2) set.seed(123) (km.res &lt;- kmeans(df, centers = 4, nstart = 25)) ## K-means clustering with 4 clusters of sizes 8, 13, 16, 13 ## ## Cluster means: ## Murder Assault UrbanPop ## 1 1.4118898 0.8743346 -0.8145211 ## 2 -0.9615407 -1.1066010 -0.9301069 ## 3 -0.4894375 -0.3826001 0.5758298 ## 4 0.6950701 1.0394414 0.7226370 ## Rape ## 1 0.01927104 ## 2 -0.96676331 ## 3 -0.26165379 ## 4 1.27693964 ## ## Clustering vector: ## Alabama Alaska ## 1 4 ## Arizona Arkansas ## 4 1 ## California Colorado ## 4 4 ## Connecticut Delaware ## 3 3 ## Florida Georgia ## 4 1 ## Hawaii Idaho ## 3 2 ## Illinois Indiana ## 4 3 ## Iowa Kansas ## 2 3 ## Kentucky Louisiana ## 2 1 ## Maine Maryland ## 2 4 ## Massachusetts Michigan ## 3 4 ## Minnesota Mississippi ## 2 1 ## Missouri Montana ## 4 2 ## Nebraska Nevada ## 2 4 ## New Hampshire New Jersey ## 2 3 ## New Mexico New York ## 4 4 ## North Carolina North Dakota ## 1 2 ## Ohio Oklahoma ## 3 3 ## Oregon Pennsylvania ## 3 3 ## Rhode Island South Carolina ## 3 1 ## South Dakota Tennessee ## 2 1 ## Texas Utah ## 4 3 ## Vermont Virginia ## 2 3 ## Washington West Virginia ## 3 2 ## Wisconsin Wyoming ## 2 3 ## ## Within cluster sum of squares by cluster: ## [1] 8.316061 11.952463 16.212213 19.922437 ## (between_SS / total_SS = 71.2 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; ## [3] &quot;totss&quot; &quot;withinss&quot; ## [5] &quot;tot.withinss&quot; &quot;betweenss&quot; ## [7] &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; There are 7 Avaliable components you can access it. Compute the mean of each variables by clustering using the original data: aggregate(USArrests, by=list(cluster=km.res$cluster), mean) ## cluster Murder Assault UrbanPop ## 1 1 13.93750 243.62500 53.75000 ## 2 2 3.60000 78.53846 52.07692 ## 3 3 5.65625 138.87500 73.87500 ## 4 4 10.81538 257.38462 76.00000 ## Rape ## 1 21.41250 ## 2 12.17692 ## 3 18.78125 ## 4 33.19231 Display results with your cluster result: dd &lt;- cbind(USArrests, cluster = km.res$cluster) head(dd) ## Murder Assault UrbanPop Rape ## Alabama 13.2 236 58 21.2 ## Alaska 10.0 263 48 44.5 ## Arizona 8.1 294 80 31.0 ## Arkansas 8.8 190 50 19.5 ## California 9.0 276 91 40.6 ## Colorado 7.9 204 78 38.7 ## cluster ## Alabama 1 ## Alaska 4 ## Arizona 4 ## Arkansas 1 ## California 4 ## Colorado 4 Plot the clustering result using factoextra package library(factoextra) fviz_cluster(km.res, df, palette=&quot;Set2&quot;, ggtheme = theme_minimal()) #points(km.res$centers, col = 1:2, pch = 8, cex=2) Reference: here 3.2 Case 2 Use iris data for a second demo str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... We use the features with “Length” to do a kmeans clustering: i &lt;- grep(&quot;Length&quot;, names(iris)) x &lt;- iris[, i] cl &lt;- kmeans(x, centers = 3, nstart = 10) plot(x, col = cl$cluster) 3.3 Hierarchical clustering d &lt;- dist(iris[, 1:4]) # calculate the euclidean distance hcl &lt;- hclust(d) hcl ## ## Call: ## hclust(d = d) ## ## Cluster method : complete ## Distance : euclidean ## Number of objects: 150 plot(hcl) Usually, you will need to cut the tree to define the clusters with cutree(). (cutree(hcl, h=1.5)) # at certain height ## [1] 1 1 1 1 1 2 1 1 1 1 2 2 1 1 2 2 2 1 2 ## [20] 2 2 2 1 2 2 1 2 1 1 1 1 2 2 2 1 1 2 1 ## [39] 1 1 1 1 1 2 2 1 2 1 2 1 3 3 3 4 3 4 3 ## [58] 5 3 4 5 4 4 3 4 3 4 4 3 4 6 4 6 3 3 3 ## [77] 3 3 3 4 4 4 4 6 4 3 3 3 4 4 4 3 4 5 4 ## [96] 4 4 3 5 4 7 6 8 7 7 8 4 8 7 8 7 6 7 6 ## [115] 6 7 7 8 8 3 7 6 8 6 7 8 6 6 7 8 8 8 7 ## [134] 6 6 8 7 7 6 7 7 7 6 7 7 7 6 7 7 6 cutree(hcl, k=2) # with certain number of clusters ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [20] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [39] 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 1 2 1 2 ## [58] 1 2 1 1 1 1 2 1 2 1 1 2 1 2 1 2 2 2 2 ## [77] 2 2 2 1 1 1 1 2 1 2 2 2 1 1 1 2 1 1 1 ## [96] 1 1 2 1 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 ## [115] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [134] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 plot(hcl) abline(h = 3.9, col=&quot;blue&quot;) You can check the result using identical() identical(cutree(hcl, k=3), cutree(hcl, h=3.9)) ## [1] TRUE "],["knn-k-nearest-neighbors.html", "Chapter 4 kNN (k-Nearest Neighbors) 4.1 Difference calculating the distance: (1) Euclidean (2) Manhatten (3) Cosine (4) Jaccard Coefficient (5) MinKowski 4.2 Find the k nearest neighbors", " Chapter 4 kNN (k-Nearest Neighbors) 2021-12-25 updated Ref: distance Ref: tutorial 4.1 Difference calculating the distance: (1) Euclidean (2) Manhatten (3) Cosine (4) Jaccard Coefficient (5) MinKowski # Euclidean distance euclidean_distance &lt;- function(a, b) { # We check that they have the same number of observation if (length(a) == length(b)) { sqrt(sum((a-b)^2)) } else { stop(&quot;Vectors must be of the same length&quot;) } } euclidean_distance(1:10, 11:20) ## [1] 31.62278 # Manhattan distance manhattan_distance &lt;- function(a, b) { # We check that they have the same number of observation if (length(a) == length(b)) { sum(abs(a-b)) } else { stop(&quot;Vectors must be of the same length&quot;) } } manhattan_distance(1:10, 11:20) ## [1] 100 # Cosine similarity cos_similarity &lt;- function(a, b) { if (length(a) == length(b)) { num = sum(a * b, na.rm = T) den = sqrt(sum(a^2, na.rm = T)) * sqrt(sum(b^2, na.rm = T)) result = num/den 1 - result # because cos(0)=1 } else { stop (1:10, 11:20) } } cos_similarity(1:10, 11:20) ## [1] 0.0440877 # measure the degree of similarity of two vectors # all values are equal = 1 # all values are different = 0 jaccard &lt;- function(a, b) { if (length(a) == length(b)) { intersection &lt;- length(intersect(a,b)) union &lt;- length(a) + length(b) - intersection intersection/union } else { stop(&quot;Vectors must be of the same length&quot;) } } jaccard(1:10, 11:20) ## [1] 0 minkowski_distance &lt;- function(a, b, p) { # p=1, Manhattan distance # p=2, Euclidean distance if (p &lt;= 0) { stop(&quot;p must be higher than 0&quot;) } if (length(a) == length(b)) { sum(abs(a-b)^p)^(1/p) } else { stop(&quot;Vectors must be of the same length&quot;) } } (minkowski_distance(1:10, 11:20, 1)) ## [1] 100 (minkowski_distance(1:10, 11:20, 2)) ## [1] 31.62278 We need to based on the type of data, the dimensions, and the business objective to decide which method we are going to use. For example, Manhattan is good for the closet route that a taxi must take. 4.2 Find the k nearest neighbors The process includes: (1) Check the number of observations is the same (2) Calculate distance (3) Find the closest neighbors. In the following, we use Boston house price data to demo KNN: library(MASS) data(Boston) str(Boston) ## &#39;data.frame&#39;: 506 obs. of 14 variables: ## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... ## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... ## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... ## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... ## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... ## $ rm : num 6.58 6.42 7.18 7 7.15 ... ## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... ## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... ## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... ## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... ## $ black : num 397 397 393 395 397 ... ## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... ## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... library(caret) set.seed(1) model &lt;- train( medv ~ ., data = Boston, method = &quot;knn&quot; ) model ## k-Nearest Neighbors ## ## 506 samples ## 13 predictor ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 506, 506, 506, 506, 506, 506, ... ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 5 6.774213 0.4788519 4.616781 ## 7 6.709875 0.4771239 4.635036 ## 9 6.746559 0.4654866 4.690258 ## ## RMSE was used to select the optimal ## model using the smallest value. ## The final value used for the model was k ## = 7. plot(model) Caret provide preprocessing method before we run our data. set.seed(1) model2 &lt;- train( medv ~ ., data = Boston, method = &quot;knn&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;) ) model2 ## k-Nearest Neighbors ## ## 506 samples ## 13 predictor ## ## Pre-processing: centered (13), scaled (13) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 506, 506, 506, 506, 506, 506, ... ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 5 4.827696 0.7297751 3.048151 ## 7 4.793191 0.7373525 3.043650 ## 9 4.788986 0.7410578 3.070081 ## ## RMSE was used to select the optimal ## model using the smallest value. ## The final value used for the model was k ## = 9. Splitting the dataset (prevent overfitting) set.seed(1) inTraining &lt;- createDataPartition(Boston$medv, p = .80, list = FALSE) training &lt;- Boston[inTraining, ] testing &lt;- Boston[-inTraining, ] model3 &lt;- train( medv ~., data = training, method = &quot;knn&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;) ) model3 ## k-Nearest Neighbors ## ## 407 samples ## 13 predictor ## ## Pre-processing: centered (13), scaled (13) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 407, 407, 407, 407, 407, 407, ... ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 5 4.948949 0.7111926 3.228177 ## 7 5.008726 0.7072326 3.240362 ## 9 5.049853 0.7042396 3.281286 ## ## RMSE was used to select the optimal ## model using the smallest value. ## The final value used for the model was k ## = 5. See the performance of our model test.features &lt;- subset(testing, select=-c(medv)) test.target &lt;- subset(testing, select=medv)[,1] predictions = predict(model3, newdata = test.features) # RMSE (sqrt(mean((test.target - predictions)^2))) ## [1] 4.563839 # R squared (cor(test.target, predictions) ^ 2 ) ## [1] 0.7814887 Cross validation set.seed(1) ctrl &lt;- trainControl( method = &quot;cv&quot;, number = 10 ) model4 &lt;- train( medv ~ ., data = training, method = &quot;knn&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), trControl = ctrl ) (model4) ## k-Nearest Neighbors ## ## 407 samples ## 13 predictor ## ## Pre-processing: centered (13), scaled (13) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 367, 366, 367, 366, 365, 367, ... ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 5 4.616138 0.7518673 3.064657 ## 7 4.734625 0.7404093 3.151517 ## 9 4.677503 0.7508160 3.156651 ## ## RMSE was used to select the optimal ## model using the smallest value. ## The final value used for the model was k ## = 5. plot(model4) # To see if model4 is better than model3 test.features &lt;- subset(testing, select=-c(medv)) test.target &lt;- subset(testing, select=medv)[,1] predictions = predict(model4, newdata = test.features) # RMSE (sqrt(mean((test.target - predictions)^2))) ## [1] 4.563839 # R squared (cor(test.target, predictions) ^ 2 ) ## [1] 0.7814887 We use lambda to tune the hyper parameters set.seed(1) tuneGrid &lt;- expand.grid( k = seq(5, 9, by = 1) ) model5 &lt;- train( medv ~., data = training, method = &quot;knn&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), trControl = ctrl, tuneGrid = tuneGrid ) (model5) ## k-Nearest Neighbors ## ## 407 samples ## 13 predictor ## ## Pre-processing: centered (13), scaled (13) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 367, 366, 367, 366, 365, 367, ... ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 5 4.616138 0.7518673 3.064657 ## 6 4.754269 0.7386282 3.162237 ## 7 4.734625 0.7404093 3.151517 ## 8 4.656271 0.7508317 3.133727 ## 9 4.677503 0.7508160 3.156651 ## ## RMSE was used to select the optimal ## model using the smallest value. ## The final value used for the model was k ## = 5. plot(model5) "],["cart-decision-tree.html", "Chapter 5 CART (Decision Tree) 5.1 Classification Tree 5.2 Random Forest", " Chapter 5 CART (Decision Tree) 2021-12-25 updated Ref: here Ref: random forest Decision Tree can handle classification and regression problem. It is also called CART (Classification and Regression Tree). In the following, we will use spam data and Boston house pricing data to demonstrate. 5.1 Classification Tree library(DAAG) # data sets used in examples and exercise library(party) # for recursive partitioning library(rpart) # recursive partitioning and regression trees library(rpart.plot) # plot rpart models library(mlbench) # collection of ML benchmark problems library(caret) # Misc functions for training/plotting models library(pROC) library(tree) # classification and regression tree # Getting Email spam data str(spam7) ## &#39;data.frame&#39;: 4601 obs. of 7 variables: ## $ crl.tot: num 278 1028 2259 191 191 ... ## $ dollar : num 0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ... ## $ bang : num 0.778 0.372 0.276 0.137 0.135 0 0.164 0 0.181 0.244 ... ## $ money : num 0 0.43 0.06 0 0 0 0 0 0.15 0 ... ## $ n000 : num 0 0.43 1.16 0 0 0 0 0 0 0.19 ... ## $ make : num 0 0.21 0.06 0 0 0 0 0 0.15 0.06 ... ## $ yesno : Factor w/ 2 levels &quot;n&quot;,&quot;y&quot;: 2 2 2 2 2 2 2 2 2 2 ... # Data partition set.seed(1234) mydata &lt;- spam7 ind &lt;- sample(2, nrow(mydata), replace=TRUE, prob=c(0.5,0.5)) # assign index train &lt;- mydata[ind == 1,] test &lt;- mydata[ind == 2,] tree &lt;- rpart(yesno ~ ., data=train) rpart.plot(tree) printcp(tree) ## ## Classification tree: ## rpart(formula = yesno ~ ., data = train) ## ## Variables actually used in tree construction: ## [1] bang crl.tot dollar ## ## Root node error: 900/2305 = 0.39046 ## ## n= 2305 ## ## CP nsplit rel error xerror ## 1 0.474444 0 1.00000 1.00000 ## 2 0.074444 1 0.52556 0.56556 ## 3 0.010000 3 0.37667 0.42111 ## xstd ## 1 0.026024 ## 2 0.022128 ## 3 0.019773 plotcp(tree) # print confusion matrix of your model on training dataset p &lt;- predict(tree, train, type = &quot;class&quot;) confusionMatrix(p, train$yesno, positive=&quot;y&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction n y ## n 1278 212 ## y 127 688 ## ## Accuracy : 0.8529 ## 95% CI : (0.8378, 0.8671) ## No Information Rate : 0.6095 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.6857 ## ## Mcnemar&#39;s Test P-Value : 5.061e-06 ## ## Sensitivity : 0.7644 ## Specificity : 0.9096 ## Pos Pred Value : 0.8442 ## Neg Pred Value : 0.8577 ## Prevalence : 0.3905 ## Detection Rate : 0.2985 ## Detection Prevalence : 0.3536 ## Balanced Accuracy : 0.8370 ## ## &#39;Positive&#39; Class : y ## # ROC curve p1 &lt;- predict(tree, test, type=&quot;prob&quot;) p1 &lt;- p1[,2] # extract y information r &lt;- multiclass.roc(test$yesno, p1, percent = TRUE) roc &lt;- r[[&quot;rocs&quot;]] r1 &lt;- roc[[1]] plot.roc( r1, print.auc = TRUE, auc.polygon = TRUE, grid = c(0.1, 0.2), grid.col = c(&quot;green&quot;, &quot;red&quot;), max.auc.polygon = TRUE, auc.polygon.col = &quot;lightblue&quot;, print.thres = TRUE, main = &quot;ROC Curve&quot; ) ## Regression Tree data(&quot;BostonHousing&quot;) mydata &lt;- BostonHousing str(mydata) ## &#39;data.frame&#39;: 506 obs. of 14 variables: ## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... ## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... ## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... ## $ chas : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... ## $ rm : num 6.58 6.42 7.18 7 7.15 ... ## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... ## $ rad : num 1 2 2 3 3 3 5 5 5 5 ... ## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... ## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... ## $ b : num 397 397 393 395 397 ... ## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... ## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... # Data partition set.seed(1234) ind &lt;- sample(2, nrow(mydata), replace = T, prob = c(0.5, 0.5)) train &lt;- mydata[ind == 1,] test &lt;- mydata[ind == 2,] tree &lt;- rpart(medv ~., data = train) rpart.plot(tree) printcp(tree) ## ## Regression tree: ## rpart(formula = medv ~ ., data = train) ## ## Variables actually used in tree construction: ## [1] age crim lstat rm ## ## Root node error: 22620/262 = 86.334 ## ## n= 262 ## ## CP nsplit rel error xerror ## 1 0.469231 0 1.00000 1.01139 ## 2 0.128700 1 0.53077 0.62346 ## 3 0.098630 2 0.40207 0.51042 ## 4 0.033799 3 0.30344 0.42674 ## 5 0.028885 4 0.26964 0.39232 ## 6 0.028018 5 0.24075 0.37848 ## 7 0.015141 6 0.21274 0.34877 ## 8 0.010000 7 0.19760 0.33707 ## xstd ## 1 0.115186 ## 2 0.080154 ## 3 0.076055 ## 4 0.069827 ## 5 0.066342 ## 6 0.066389 ## 7 0.065824 ## 8 0.065641 plotcp(tree) # predict p &lt;- predict(tree, train) # RMSE (sqrt(mean((train$medv - p)^2))) ## [1] 4.130294 # R squared (cor(train$medv, p))^2 ## [1] 0.8024039 In the regression model, the r square value is 80% and RMSE is 4.13, not bad at all.. In this way, you can make use of Decision classification regression tree models. 5.2 Random Forest library(mlbench) data(Sonar) (head(Sonar)) ## V1 V2 V3 V4 V5 ## 1 0.0200 0.0371 0.0428 0.0207 0.0954 ## 2 0.0453 0.0523 0.0843 0.0689 0.1183 ## 3 0.0262 0.0582 0.1099 0.1083 0.0974 ## 4 0.0100 0.0171 0.0623 0.0205 0.0205 ## 5 0.0762 0.0666 0.0481 0.0394 0.0590 ## 6 0.0286 0.0453 0.0277 0.0174 0.0384 ## V6 V7 V8 V9 V10 ## 1 0.0986 0.1539 0.1601 0.3109 0.2111 ## 2 0.2583 0.2156 0.3481 0.3337 0.2872 ## 3 0.2280 0.2431 0.3771 0.5598 0.6194 ## 4 0.0368 0.1098 0.1276 0.0598 0.1264 ## 5 0.0649 0.1209 0.2467 0.3564 0.4459 ## 6 0.0990 0.1201 0.1833 0.2105 0.3039 ## V11 V12 V13 V14 V15 ## 1 0.1609 0.1582 0.2238 0.0645 0.0660 ## 2 0.4918 0.6552 0.6919 0.7797 0.7464 ## 3 0.6333 0.7060 0.5544 0.5320 0.6479 ## 4 0.0881 0.1992 0.0184 0.2261 0.1729 ## 5 0.4152 0.3952 0.4256 0.4135 0.4528 ## 6 0.2988 0.4250 0.6343 0.8198 1.0000 ## V16 V17 V18 V19 V20 ## 1 0.2273 0.3100 0.2999 0.5078 0.4797 ## 2 0.9444 1.0000 0.8874 0.8024 0.7818 ## 3 0.6931 0.6759 0.7551 0.8929 0.8619 ## 4 0.2131 0.0693 0.2281 0.4060 0.3973 ## 5 0.5326 0.7306 0.6193 0.2032 0.4636 ## 6 0.9988 0.9508 0.9025 0.7234 0.5122 ## V21 V22 V23 V24 V25 ## 1 0.5783 0.5071 0.4328 0.5550 0.6711 ## 2 0.5212 0.4052 0.3957 0.3914 0.3250 ## 3 0.7974 0.6737 0.4293 0.3648 0.5331 ## 4 0.2741 0.3690 0.5556 0.4846 0.3140 ## 5 0.4148 0.4292 0.5730 0.5399 0.3161 ## 6 0.2074 0.3985 0.5890 0.2872 0.2043 ## V26 V27 V28 V29 V30 ## 1 0.6415 0.7104 0.8080 0.6791 0.3857 ## 2 0.3200 0.3271 0.2767 0.4423 0.2028 ## 3 0.2413 0.5070 0.8533 0.6036 0.8514 ## 4 0.5334 0.5256 0.2520 0.2090 0.3559 ## 5 0.2285 0.6995 1.0000 0.7262 0.4724 ## 6 0.5782 0.5389 0.3750 0.3411 0.5067 ## V31 V32 V33 V34 V35 ## 1 0.1307 0.2604 0.5121 0.7547 0.8537 ## 2 0.3788 0.2947 0.1984 0.2341 0.1306 ## 3 0.8512 0.5045 0.1862 0.2709 0.4232 ## 4 0.6260 0.7340 0.6120 0.3497 0.3953 ## 5 0.5103 0.5459 0.2881 0.0981 0.1951 ## 6 0.5580 0.4778 0.3299 0.2198 0.1407 ## V36 V37 V38 V39 V40 ## 1 0.8507 0.6692 0.6097 0.4943 0.2744 ## 2 0.4182 0.3835 0.1057 0.1840 0.1970 ## 3 0.3043 0.6116 0.6756 0.5375 0.4719 ## 4 0.3012 0.5408 0.8814 0.9857 0.9167 ## 5 0.4181 0.4604 0.3217 0.2828 0.2430 ## 6 0.2856 0.3807 0.4158 0.4054 0.3296 ## V41 V42 V43 V44 V45 ## 1 0.0510 0.2834 0.2825 0.4256 0.2641 ## 2 0.1674 0.0583 0.1401 0.1628 0.0621 ## 3 0.4647 0.2587 0.2129 0.2222 0.2111 ## 4 0.6121 0.5006 0.3210 0.3202 0.4295 ## 5 0.1979 0.2444 0.1847 0.0841 0.0692 ## 6 0.2707 0.2650 0.0723 0.1238 0.1192 ## V46 V47 V48 V49 V50 ## 1 0.1386 0.1051 0.1343 0.0383 0.0324 ## 2 0.0203 0.0530 0.0742 0.0409 0.0061 ## 3 0.0176 0.1348 0.0744 0.0130 0.0106 ## 4 0.3654 0.2655 0.1576 0.0681 0.0294 ## 5 0.0528 0.0357 0.0085 0.0230 0.0046 ## 6 0.1089 0.0623 0.0494 0.0264 0.0081 ## V51 V52 V53 V54 V55 ## 1 0.0232 0.0027 0.0065 0.0159 0.0072 ## 2 0.0125 0.0084 0.0089 0.0048 0.0094 ## 3 0.0033 0.0232 0.0166 0.0095 0.0180 ## 4 0.0241 0.0121 0.0036 0.0150 0.0085 ## 5 0.0156 0.0031 0.0054 0.0105 0.0110 ## 6 0.0104 0.0045 0.0014 0.0038 0.0013 ## V56 V57 V58 V59 V60 Class ## 1 0.0167 0.0180 0.0084 0.0090 0.0032 R ## 2 0.0191 0.0140 0.0049 0.0052 0.0044 R ## 3 0.0244 0.0316 0.0164 0.0095 0.0078 R ## 4 0.0073 0.0050 0.0044 0.0040 0.0117 R ## 5 0.0015 0.0072 0.0048 0.0107 0.0094 R ## 6 0.0089 0.0057 0.0027 0.0051 0.0062 R set.seed(12) model &lt;- train( Class ~., data = Sonar, method = &quot;ranger&quot; ) print(model) ## Random Forest ## ## 208 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 208, 208, 208, 208, 208, 208, ... ## Resampling results across tuning parameters: ## ## mtry splitrule Accuracy Kappa ## 2 gini 0.8078699 0.6108857 ## 2 extratrees 0.8188579 0.6341083 ## 31 gini 0.7751013 0.5455448 ## 31 extratrees 0.8275348 0.6496807 ## 60 gini 0.7676651 0.5302055 ## 60 extratrees 0.8221403 0.6387246 ## ## Tuning parameter &#39;min.node.size&#39; was ## held constant at a value of 1 ## Accuracy was used to select the ## optimal model using the largest value. ## The final values used for the model ## were mtry = 31, splitrule = ## extratrees and min.node.size = 1. plot(model) Tune hyperparameters: set.seed(42) myGrid &lt;- expand.grid( mtry = c(5, 10, 20, 40, 60), splitrule=c(&quot;gini&quot;, &quot;extratrees&quot;), min.node.size = 1 ) model &lt;- train( Class ~ ., data = Sonar, method = &quot;ranger&quot;, tuneGrid = myGrid, trControl = trainControl( method = &quot;cv&quot;, number = 5, verboseIter = FALSE ) ) print(model) ## Random Forest ## ## 208 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 166, 167, 167, 167, 165 ## Resampling results across tuning parameters: ## ## mtry splitrule Accuracy Kappa ## 5 gini 0.8173838 0.6297111 ## 5 extratrees 0.8273668 0.6497566 ## 10 gini 0.8171569 0.6305735 ## 10 extratrees 0.8414310 0.6791146 ## 20 gini 0.7978716 0.5908457 ## 20 extratrees 0.8417740 0.6792866 ## 40 gini 0.7879994 0.5727742 ## 40 extratrees 0.8466467 0.6897164 ## 60 gini 0.7928774 0.5822691 ## 60 extratrees 0.8608271 0.7187262 ## ## Tuning parameter &#39;min.node.size&#39; was ## held constant at a value of 1 ## Accuracy was used to select the ## optimal model using the largest value. ## The final values used for the model ## were mtry = 60, splitrule = ## extratrees and min.node.size = 1. plot(model) "],["model-selection.html", "Chapter 6 Model selection 6.1 Linear model", " Chapter 6 Model selection Ref: here In this section, we demonstrate how to compare the performance between different models. So the first step is to create a training and testing dataset. library(C50) data(&quot;mlc_churn&quot;) table(mlc_churn$churn)/nrow(mlc_churn) ## ## yes no ## 0.1414 0.8586 We see that about 15% of the customers churn. It is important to maintain this proportion in all of the folds. myFolds &lt;- createFolds(mlc_churn$churn, k=5) str(myFolds) ## List of 5 ## $ Fold1: int [1:1000] 4 9 18 25 30 33 42 47 67 73 ... ## $ Fold2: int [1:1000] 1 3 5 11 14 23 36 44 55 61 ... ## $ Fold3: int [1:999] 8 12 15 22 24 27 28 29 31 32 ... ## $ Fold4: int [1:1000] 6 13 16 19 26 38 39 46 48 50 ... ## $ Fold5: int [1:1001] 2 7 10 17 20 21 34 35 37 41 ... # verify sapply(myFolds, function(i){ table(mlc_churn$churn[i])/length(i) }) ## Fold1 Fold2 Fold3 Fold4 Fold5 ## yes 0.142 0.141 0.1411411 0.141 0.1418581 ## no 0.858 0.859 0.8588589 0.859 0.8581419 myControl &lt;- trainControl( summaryFunction = twoClassSummary, classProb = TRUE, verboseIter = FALSE, savePredictions = TRUE, index = myFolds ) 6.1 Linear model glm_model &lt;- train( churn ~., mlc_churn, metric = &quot;ROC&quot;, method = &quot;glmnet&quot;, tuneGrid = expand.grid( alpha = 0:1, lambda = 0:10/10 ), trControl = myControl ) print(glm_model) ## glmnet ## ## 5000 samples ## 19 predictor ## 2 classes: &#39;yes&#39;, &#39;no&#39; ## ## No pre-processing ## Resampling: Bootstrapped (5 reps) ## Summary of sample sizes: 1000, 1000, 999, 1000, 1001 ## Resampling results across tuning parameters: ## ## alpha lambda ROC Sens ## 0 0.0 0.7810206 0.2301791801 ## 0 0.1 0.7914828 0.0654123018 ## 0 0.2 0.7915574 0.0180324588 ## 0 0.3 0.7907987 0.0067194096 ## 0 0.4 0.7900281 0.0003533569 ## 0 0.5 0.7893529 0.0000000000 ## 0 0.6 0.7887966 0.0000000000 ## 0 0.7 0.7883022 0.0000000000 ## 0 0.8 0.7878990 0.0000000000 ## 0 0.9 0.7875657 0.0000000000 ## 0 1.0 0.7872430 0.0000000000 ## 1 0.0 0.7606466 0.2673122987 ## 1 0.1 0.5413578 0.0000000000 ## 1 0.2 0.5000000 0.0000000000 ## 1 0.3 0.5000000 0.0000000000 ## 1 0.4 0.5000000 0.0000000000 ## 1 0.5 0.5000000 0.0000000000 ## 1 0.6 0.5000000 0.0000000000 ## 1 0.7 0.5000000 0.0000000000 ## 1 0.8 0.5000000 0.0000000000 ## 1 0.9 0.5000000 0.0000000000 ## 1 1.0 0.5000000 0.0000000000 ## Spec ## 0.9685529 ## 0.9957487 ## 0.9996506 ## 1.0000000 ## 1.0000000 ## 1.0000000 ## 1.0000000 ## 1.0000000 ## 1.0000000 ## 1.0000000 ## 1.0000000 ## 0.9584782 ## 1.0000000 ## 1.0000000 ## 1.0000000 ## 1.0000000 ## 1.0000000 ## 1.0000000 ## 1.0000000 ## 1.0000000 ## 1.0000000 ## 1.0000000 ## ## ROC was used to select the optimal ## model using the largest value. ## The final values used for the model ## were alpha = 0 and lambda = 0.2. plot(glm_model) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
