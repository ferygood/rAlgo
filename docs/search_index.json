[["ab-test.html", "Chapter 2 A/B test 2.1 Scenario 2.2 Case study", " Chapter 2 A/B test 2021-12-05 updated A/B test is used for testing which version of product can give you a preferable outcome by setting two versions of a product/test with only one variable is different (which is version A and version B). A is usually the “current version” and B is often the “improved version”. 2.1 Scenario For someone who wants to test new feature on product. A: Offer ends this Saturday! Please use promotion code A B: The offer is about to end, please use promotion code B Send these message to a population and see which one get more clicks or sell more products. For a more persuasive result, a statistical test should be used to test if it is a significant result. 2.2 Case study ref In short, we want to test two variant design on a hotel booking website. Based on result, make recommendation to your stakeholders. library(tidyverse) urlfile &lt;- &quot;https://raw.githubusercontent.com/etomaa/A-B-Testing/master/data/Website%20Results.csv&quot; df &lt;- read_csv(url(urlfile), show_col_types = FALSE) glimpse(df) ## Rows: 1,451 ## Columns: 4 ## $ variant &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;… ## $ converted &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… ## $ length_of_stay &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0,… ## $ revenue &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,… variant A: current website (control group) variant B: experimental group (to see if new feature increase the conversion) converted: if TRUE, it means that the customer completes bookings and it’s going to show FALSE when the customer visits the sites but not makes a booking. Test Hypothesis: - Null hypothesis: Pcont_A = Pcont_B - Alternative hypothesis: Pcont_A != Pcont_B # conversion rate A conversion_subset_A &lt;- df %&gt;% filter(variant == &quot;A&quot; &amp; converted == &quot;TRUE&quot;) conversions_A &lt;- nrow(conversion_subset_A) visitors_A &lt;- nrow(df %&gt;% filter(variant == &quot;A&quot;)) conv_rate_A &lt;- (conversions_A/visitors_A) # conversion rate B conversion_subset_B &lt;- df %&gt;% filter(variant == &quot;B&quot; &amp; converted == &quot;TRUE&quot;) conversions_B &lt;- nrow(conversion_subset_B) visitors_B &lt;- nrow(df %&gt;% filter(variant == &quot;B&quot;)) conv_rate_B &lt;- (conversions_B/visitors_B) print(paste0(&quot;conversion rates of A and B are &quot;, round(conv_rate_A, 4), &quot; and &quot;, round(conv_rate_B, 4))) ## [1] &quot;conversion rates of A and B are 0.0277 and 0.0507&quot; Then we compute the relative uplift using the conversion rates A &amp; B. uplift &lt;- (conv_rate_B - conv_rate_A) / conv_rate_A * 100 uplift ## [1] 82.71918 B is better 83% than A. This is high enough to decide a winner. However, we need to use statistical methods to give a more precise result. # Pooled sample proportion for variants A &amp; B p_pool &lt;- (conversions_A + conversions_B) / (visitors_A + visitors_B) # Let&#39;s compute Standard error for variants A &amp; B (SE_pool) SE_pool &lt;- sqrt(p_pool * (1 - p_pool) * ((1 / visitors_A) + (1 / visitors_B))) # Let&#39;s compute the margin of error for the pool MOE &lt;- SE_pool * qnorm(0.975) # Point Estimate or Difference in proportion d_hat &lt;- conv_rate_B - conv_rate_A # compute the Z-score so we can determine the p-value z_score &lt;- d_hat / SE_pool cat(paste0(&quot;The pooled probabilty is &quot;, round(p_pool, 4), &quot;.\\nStandard error is &quot;, round(SE_pool, 4), &quot;.\\nMargin of error is &quot;, round(MOE, 4), &quot;.\\nDifference in proportion of variants A &amp; B is &quot;, round(d_hat,4), &quot;.\\nZ score is &quot;, round(z_score, 4))) ## The pooled probabilty is 0.0393. ## Standard error is 0.0102. ## Margin of error is 0.02. ## Difference in proportion of variants A &amp; B is 0.0229. ## Z score is 2.2495 Using Z-score, we can quickly determine the p-value via a look-up table, or using code below. Also conpute the confidence interval for the pool p_value &lt;- pnorm(q = -z_score, mean = 0, sd = 1) * 2 ci &lt;- c(d_hat - MOE, d_hat + MOE) # confidence interval of variant A X_hat_A &lt;- conversions_A / visitors_A se_hat_A &lt;- sqrt(X_hat_A * (1 - X_hat_A) / visitors_A) ci_A &lt;- c(X_hat_A - qnorm(0.975) * se_hat_A, X_hat_A + qnorm(0.975) * se_hat_A) # confidence interval of variant B X_hat_B &lt;- conversions_B / visitors_B se_hat_B &lt;- sqrt(X_hat_B * (1 - X_hat_B) / visitors_B) ci_B &lt;- c(X_hat_B - qnorm(0.975) * se_hat_B, X_hat_B + qnorm(0.975) * se_hat_B) cat(paste0(&quot;p value is &quot;, round(p_value, 4), &quot;\\nConfidence interval is &quot;, list(round(ci, 4)), &quot;\\nConfidence interval of A is &quot;, list(round(ci_A, 4)),&quot;\\nConfidence interval of B is &quot;, list(round(ci_B, 4)))) ## p value is 0.0245 ## Confidence interval is c(0.003, 0.0429) ## Confidence interval of A is c(0.0158, 0.0397) ## Confidence interval of B is c(0.0348, 0.0666) Show the computation result in table: vis_result_pool &lt;- data.frame( metric = c( &#39;Estimated Difference&#39;, &#39;Relative Uplift(%)&#39;, &#39;pooled sample proportion&#39;, &#39;Standard Error of Difference&#39;, &#39;z_score&#39;, &#39;p-value&#39;, &#39;Margin of Error&#39;, &#39;CI-lower&#39;, &#39;CI-upper&#39;), value = c( conv_rate_B - conv_rate_A, uplift, p_pool, SE_pool, z_score, p_value, MOE, ci[1], ci[2] )) vis_result_pool ## metric value ## 1 Estimated Difference 0.022945680 ## 2 Relative Uplift(%) 82.719178082 ## 3 pooled sample proportion 0.039283253 ## 4 Standard Error of Difference 0.010200138 ## 5 z_score 2.249546089 ## 6 p-value 0.024477774 ## 7 Margin of Error 0.019991903 ## 8 CI-lower 0.002953777 ## 9 CI-upper 0.042937584 Recommendation and Conclusion * Variant A has 20 conversions and 721 hits whereas Variant B has 37 conversions and 730 hits. * Relative uplift of 82.72% based on a variant A conversion rate is 2.77% and for B is 5.07%. Hence, variant B is better than A by 82.72%. * For this analysis P-value computed was 0.02448. Hence, there is strong statistical significance in test results. * From the above results that depict strong statistical significance. You should reject the null hypothesis and proceed with the launch. * Therefore, Accept Variant B and you can roll it to the users for 100%. Limitations It is one of the tools for conversion optimization and it’s not an independent solution and it’s not going to fix all the conversion issues of ours and it can’t fix the issues as you get with messy data and you need to perform more than just an A/B test to improve on conversions. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
